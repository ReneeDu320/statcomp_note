{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Dataset\n",
    "\n",
    "\n",
    "### Woking with package data.table in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data.table is a mature package for fast data processing that presents an alternative to dplyr.\n",
    "\n",
    "#### Read Data\n",
    " data.table supports rolling joins (which allow rows in one table to be selected based on proximity between shared variables (typically time) and non-equi joins (where join criteria can be inequalities rather than equal to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open files\n",
    "# using fread()\n",
    "library(data.table)\n",
    "n <- 10000\n",
    "file <- \"CollegeScorecard_Raw_Data/Scorecard_2009-2016.csv\"\n",
    "system.time({\n",
    "  scores <- fread(file, nrows=n)\n",
    "})\n",
    "\n",
    "##    user  system elapsed \n",
    "##   4.616   0.120   4.737\n",
    "\n",
    "\n",
    "system.time({\n",
    "  scores2 <- read.csv(file, nrows=n)\n",
    "})\n",
    "##    user  system elapsed \n",
    "##  15.849   0.096  15.946"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Manipulation\n",
    "Here, NUMBRANCH is a column of the dataset and using data.table we can pull out certain rows by invoking the column name without having to write scores$NUMBRANCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "z <- scores[NUMBRANCH > 25]\n",
    "nrow(z)\n",
    "#253\n",
    "\n",
    "scores[INSTNM == \"University of North Carolina at Chapel Hill\",1:20]\n",
    "\n",
    "'''\n",
    "We can also specify a column to be a key for the data.table. \n",
    "Specifying a key allows very fast subsetting based on the column you specify. Here, because the key is an integer, we wrap up the key in .(), \n",
    "otherwise it would interpret our request as a row number:\n",
    "'''\n",
    "setkey(scores, UNITID)\n",
    "scores[.(199120),1:20] # 20 columns\n",
    "\n",
    "setkey(scores, CITY)\n",
    "scores[\"Chapel Hill\",1:20]\n",
    "##    UNITID    OPEID OPEID6                                      INSTNM\n",
    "## 1: 199120 00297400   2974 University of North Carolina at Chapel Hill\n",
    "## 2: 455141 04140700  41407                 Aveda Institute-Chapel Hill\n",
    "##           CITY STABBR        ZIP ACCREDAGENCY INSTURL NPCURL SCH_DEG HCM2\n",
    "## 1: Chapel Hill     NC      27599         NULL    NULL   NULL       3 NULL\n",
    "## 2: Chapel Hill     NC 27514-7001         NULL    NULL   NULL       1 NULL\n",
    "##    MAIN NUMBRANCH PREDDEG HIGHDEG CONTROL ST_FIPS REGION LOCALE\n",
    "## 1:    1         1       3       4       1      37      5   NULL\n",
    "## 2:    1         1       1       1       3      37      5   NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the key does not have to be unique (unlike row names in R which must be unique). Subsetting with a key column using data.table is much faster than subsetting via other methods. But the main takeaway should be that data.table is fast and if you have large datasets, you shouldn’t be using data.frame and base R functions for subsetting or grouping and summarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions inside the brackets\n",
    "\n",
    "We can put functions inside of the square brackets.\n",
    "\n",
    "We first convert TUITFTE to numeric, which gives a warning about NAs introduced in the coercion step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores$TUITFTE <- as.numeric(scores$TUITFTE)\n",
    "## Warning: NAs introduced by coercion\n",
    "\n",
    "# use data.table\n",
    "scores[,mean(TUITFTE,na.rm=TRUE)]\n",
    "\n",
    "#grouping operations\n",
    "scores[CONTROL==1,mean2(TUITFTE)]\n",
    "scores[,mean2(TUITFTE),by=CONTROL]\n",
    "\n",
    "#inour multiple functions\n",
    "q25 <- function(x) quantile(x, .25, na.rm=TRUE)\n",
    "q50 <- function(x) quantile(x, .50, na.rm=TRUE)\n",
    "q75 <- function(x) quantile(x, .75, na.rm=TRUE)\n",
    "scores[,.(median=q50(TUITFTE),q25=q25(TUITFTE),q75=q75(TUITFTE)),by=CONTROL]\n",
    "##    CONTROL  median     q25      q75\n",
    "## 1:       3  7621.5 4823.25 11503.25\n",
    "## 2:       1  2549.0 1381.00  4635.00\n",
    "## 3:       2 11631.5 7775.00 16654.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### working with RSQLite\n",
    "SQLite stores the entire database (definitions, tables, indices, and the data itself) as a single cross-platform file on a host machine. It implements this simple design by locking the entire database file during writing. SQLite read operations can be multitasked, though writes can only be performed sequentially.\n",
    "\n",
    "If we wanted to try out the RQLite package without writing a file to disk we could have also used \":memory:\" instead of writing a filename, which creates an in-memory database.\n",
    "\n",
    "#### use package sqldf to write queries\n",
    "library(sqldf)\n",
    "sqldf('SELECT age, circumference FROM Orange WHERE Tree = 1 ORDER BY circumference ASC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect to myDB.sqlite\n",
    "\n",
    "library(RSQLite)\n",
    "library(DBI)\n",
    "con <- dbConnect(SQLite(), \"myDB.sqlite\")\n",
    "con\n",
    "## <SQLiteConnection>\n",
    "##   Path: /home/love/teach/statcomp/statcomp_src/large/myDB.sqlite\n",
    "##   Extensions: TRUE\n",
    "\n",
    "data(mtcars)\n",
    "dbWriteTable(con, \"cars\", mtcars)\n",
    "dbListTables(con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### queries\n",
    "\n",
    "if there are more data than can fit in memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows <- dbGetQuery(con, \"SELECT * FROM cars where ...\")\n",
    "head(rows)\n",
    "\n",
    "#fetch chunks of data -- large datasets\n",
    "\n",
    "#Here we formulate a query rs, and then fetch 10 rows at a time with dbFetch:\n",
    "rs <- dbSendQuery(con, \"SELECT * FROM cars\")\n",
    "d1 <- dbFetch(rs, n=10)\n",
    "dbHasCompleted(rs)\n",
    "#we can extract all remaining data by specifying -1:\n",
    "d2 <- dbFetch(rs, n=-1)\n",
    "dbHasCompleted(rs)\n",
    "\n",
    "dbClearResult(rs)\n",
    "\n",
    "dbDisconnect(con)\n",
    "#Finally, we close the connection when we are finished working with the database:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with RHDF5\n",
    "The motivation for using an HDF5 data container is that, like SQLite we have a common format for representing a complex set of tables that can be shared simply be sharing a file, but unlike SQLite we are typically interested in reading in entire tables into memory, so that we can then analyze them. HDF5 is typically smaller on disk, as well as faster for writing or reading to or from disk, compared to SQLite.\n",
    "\n",
    "An HDF5 data container is a standardized, highly-customizable data receptacle designed for portability. Unless your definition of ‘container’ is extremely broad, file systems are not commonly considered containers.\n",
    "\n",
    "File systems aren’t portable: For example, you might be able to mount an NTFS file system on an AIX machine, but the integers or floating point numbers written on an Intel processor will turn out to be garbage when read on a IBM Power processor.\n",
    "\n",
    "HDF5 achieves portability by separating its “cargo” (data) from its environment (file system, processor architecture, etc.) and by encoding it in a self-describing file format. The HDF5 library serves the dual purpose of being a parser/encoder of this format and an API for user-level objects (datasets, groups, attributes, etc.).\n",
    "\n",
    "…\n",
    "\n",
    "The data stored in HDF5 datasets is shaped and it is typed. Datasets have (logically) the shape of multi-dimensional rectilinear arrays. All elements in a given dataset are of the same type, and HDF5 has one of the most extensive type systems and one that is user-extendable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The rhdf5 package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(\"BiocManager\") # can be skipped after 1st time\n",
    "BiocManager::install(\"rhdf5\")\n",
    "library(rhdf5)\n",
    "\n",
    "'''\n",
    "Typically, we may already have an HDF5 data container that we want to work with, \n",
    "but as in the SQLite lecture note, we will show how to create a new one first.\n",
    "'''\n",
    "h5file <- \"myDB.h5\"\n",
    "h5createFile(h5file)\n",
    "\n",
    "x <- matrix(rnorm(1e4),nrow=100)\n",
    "h5write(x, h5file, \"A/x\")\n",
    "y <- matrix(letters, nrow=13)\n",
    "h5write(y, h5file,\"A/C/y\")\n",
    "df <- data.frame(a=1L:5L,\n",
    "                 b=seq(0,1,length.out=5),\n",
    "                 c=letters[1:5],\n",
    "                 stringsAsFactors=FALSE)\n",
    "h5write(df, h5file, \"B/df\")\n",
    "h5ls(h5file)\n",
    "##   group name       otype   dclass       dim\n",
    "## 0     /    A   H5I_GROUP                   \n",
    "## 1    /A    C   H5I_GROUP                   \n",
    "## 2  /A/C    y H5I_DATASET   STRING    13 x 2\n",
    "## 3    /A    x H5I_DATASET    FLOAT 100 x 100\n",
    "## 4     /    B   H5I_GROUP                   \n",
    "## 5    /B   df H5I_DATASET COMPOUND         5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading objects\n",
    "We can read out these objects using `h5read`. Note that the column names of the data.frame have been preserved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx <- h5read(h5file, \"A/x\")\n",
    "xx[1:3,1:3]\n",
    "##            [,1]       [,2]       [,3]\n",
    "## [1,] -2.9180159 -0.3099286  0.5671834\n",
    "## [2,]  1.2320955 -1.5603322 -0.7619277\n",
    "## [3,] -0.3517632  0.2978257  0.9193802\n",
    "\n",
    "df2 <- h5read(h5file, \"B/df\")\n",
    "head(df2)\n",
    "##   a    b c\n",
    "## 1 1 0.00 a\n",
    "## 2 2 0.25 b\n",
    "## 3 3 0.50 c\n",
    "## 4 4 0.75 d\n",
    "## 5 5 1.00 e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Data\n",
    "Let’s dive right into representing sparse matrices. Here we have a large-ish matrix wherein the non-zero elements make up only ~5% of the total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m <- matrix(rbinom(1e6, 1, .05), ncol=1e3) # rbinom function (n,size,prob) n-number of observations. If length(n) > 1, the length is taken to be the number required\n",
    "m[1:5,1:5]\n",
    "##      [,1] [,2] [,3] [,4] [,5]\n",
    "## [1,]    0    0    0    0    0\n",
    "## [2,]    0    0    0    0    0\n",
    "## [3,]    0    0    0    0    0\n",
    "## [4,]    0    0    0    0    0\n",
    "## [5,]    1    0    0    0    0\n",
    "\n",
    "# Getting the number from 1 to 9\n",
    "x <- rep(1:9)\n",
    "x\n",
    "  \n",
    "# Calling the dim() function to\n",
    "# Set dimension of 3 * 3\n",
    "dim(x) <- c(3, 3)\n",
    "\n",
    "#prod() function in R Language is used to return the multiplication results of all the values present in its arguments.\n",
    "#> prod(c(1,2,4,NA,5),2:3,na.rm=TRUE)- 240， 1*2*4*5*2*3=240 \n",
    "prod(dim(m))\n",
    "## [1] 1e+06\n",
    "print(object.size(m), units=\"Mb\") #-- This matrix takes up about 4 Mb in memory\n",
    "\n",
    "#create sparse matrix\n",
    "library(Matrix)\n",
    "mm <- Matrix(m, sparse=TRUE)\n",
    "mm[1:5,1:5]\n",
    "## 5 x 5 sparse Matrix of class \"dgCMatrix\"\n",
    "##               \n",
    "## [1,] . . . . .\n",
    "## [2,] . . . . .\n",
    "## [3,] . . . . .\n",
    "## [4,] . . . . .\n",
    "## [5,] 1 . . . .\n",
    "#The sparse version takes up less than 1/6 of the space of the dense version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### how to create sparse matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s <- sparseMatrix(i=c(1,3,5),j=c(1,2,3),x=c(4,5,6),dims=list(6,4))\n",
    "## 6 x 4 sparse Matrix of class \"dgCMatrix\"\n",
    "##             \n",
    "## [1,] 4 . . .\n",
    "## [2,] . . . .\n",
    "## [3,] . 5 . .\n",
    "## [4,] . . . .\n",
    "## [5,] . . 6 .\n",
    "## [6,] . . . ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do many operations to sparse matrices using specialized functions which are different than the ones defined for regular matrices. These are described in ?dgCMatrix-class, but some of the important ones are %*%, crossprod, tcrossprod, solve, qr, lu. Using these operations will preserve the sparsity of the object (so keeping us under our memory budger), and will perform much faster than coercion to dense would, if the matrices have a high degree of sparsity.\n",
    "\n",
    "Note that some operations destroy the sparsity, such as adding 1, and therefore must be avoided (in the case where the dense matrix would not fit in memory):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[1:10,1:10] + 1\n",
    "s[1:10,1:10] * 2\n",
    "image(s[1:100,1:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### use of sparse matrices in glmnet\n",
    "\n",
    "A common use case of sparse matrices is in prediction of a target, let’s call y, using a high-dimensional, sparse matrix of features x. We are often in situation that there are more features in x than there are observations (rows of x and length of y). In this case it may make sense to first try linear modeling of y on x, and to use some combination of L1 and L2 regularization to stabilize the regression. The glmnet package allows one to fit elastic net models for such a problem, where the x matrix can be sparse, and it builds off of the sparse matrices defined in the Matrix package. Read over the help file for the main function:\n",
    "\n",
    "--overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(glmnet)\n",
    "\n",
    "n <- 1e3\n",
    "nn <- 1e5\n",
    "x <- sparseMatrix(i=sample(n,nn,TRUE),\n",
    "                  j=sample(n,nn,TRUE),\n",
    "                  dims=list(n,n))\n",
    "beta <- rep(c(1,0),c(50,950))\n",
    "y <- x %*% beta + rnorm(n,0,.25)\n",
    "fit <- glmnet(x, y, family=\"gaussian\", alpha=1)\n",
    "plot(fit)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
